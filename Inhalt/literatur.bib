% This file was created with Citavi 6.14.0.0

@collection{.,
 title = {Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18}
}


@online{.03.01.2019,
 year = {03.01.2019},
 title = {Deep Learning},
 url = {https://www.deeplearningbook.org/},
 urldate = {2023-01-04}
}


@online{.04.12.2022,
 author = {N.N.},
 title = {e-Service und Open Data},
 url = {https://www.karlsruhe.de/mobilitaet-stadtbild/bauen-und-immobilien/geoportal-karlsruhe/e-service-und-open-data},
 urldate = {2022-12-04},
 abstract = {Unser eService-Angebot besteht aus Bestellformularen, mit denen bestimmte Personengruppen -- zum Beispiel Studenten oder Firmen, die im Auftrag der Stadt arbeiten -- kostenlos Daten bestellen k{\"o}nnen.},
 organization = {Karlsruhe}
}


@article{.05.10.2021,
 author = {Olbricht, Roland M.},
 year = {2015},
 title = {Data Retrieval for Small Spatial Regions in OpenStreetMap},
 pages = {101--122},
 pagination = {page},
 journaltitle = {OpenStreetMap in GIScience: Experiences, Research, and Applications}
}


@online{.06.10.2022,
 year = {06.10.2022},
 title = {Semantic segmentation of UAV image using combined U-net and heterogeneous UAV imagery datasets | SPIE Sensors + Imaging},
 url = {https://spie.org/spie-sensing-imaging/presentation/Semantic-segmentation-of-UAV-image-using-combined-unet-and-heterogeneous/12269-18?SSO=1},
 urldate = {2022-10-06},
 abstract = {View presentations details for Semantic segmentation of UAV image using combined U-net and heterogeneous UAV imagery datasets at SPIE Sensors + Imaging}
}


@online{.10.11.2022,
 author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
 title = {Detection and Segmentation},
 url = {http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf},
 urldate = {2022-11-10},
 organization = {{Stanford University}},
 note = {CS231n: Convolutional Neural Networks for Visual Recognition}
}


@online{.17.12.2022,
 year = {17.12.2022},
 title = {NN SVG},
 url = {http://alexlenail.me/NN-SVG/LeNet.html},
 urldate = {2023-01-06}
}


@inproceedings{.20.04.2022,
 author = {Boguszewski, Adrian and Batorski, Dominik and Ziemba-Jankowska, Natalia and Dziedzic, Tomasz and Zambrzycka, Anna},
 title = {LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery},
 pages = {1102--1110},
 bookpagination = {page},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2021}
}


@collection{.2007,
 year = {2007},
 title = {University of Manchester}
}


@proceedings{.2018,
 year = {2018},
 title = {International Conference on Pattern Recognition Applications and Methods},
 isbn = {978-989-758-276-9},
 eventdate = {16/01/2018 - 18/01/2018}
}


@proceedings{.2018b,
 year = {2018},
 title = {IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
 isbn = {978-1-5386-8240-1}
}


@proceedings{.2018c,
 year = {2018},
 title = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
 isbn = {2160-7516},
 eventtitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}
}


@proceedings{.2018d,
 year = {2018},
 title = {IEEE Conference on Computer Vision and Pattern Recognition}
}


@proceedings{.2021,
 year = {2020},
 title = {25th International Conference on Pattern Recognition (ICPR)},
 isbn = {1051-4651},
 file = {2020 25th International Conference 2021:Attachments/2020 25th International Conference 2021.pdf:application/pdf},
 eventtitle = {25th International Conference on Pattern Recognition (ICPR)}
}


@proceedings{.2021b,
 year = {2021},
 title = {IEEE Conference on Computer Vision and Pattern Recognition}
}


@proceedings{.2022,
 year = {2022},
 eventtitle = {CARI 2022}
}


@online{.24.09.2022,
 year = {24.09.2022},
 title = {UAVid Semantic Segmentation Dataset},
 url = {https://uavid.nl/},
 urldate = {2022-10-06}
}


@online{.26.10.2022,
 author = {N.N.},
 title = {OpenGeoData.NI},
 url = {https://opengeodata.lgln.niedersachsen.de},
 urldate = {2022-11-20},
 organization = {Niedersachsen}
}


@inproceedings{Amiri.19.02.2020,
 author = {Amiri, Mina and Brooks, Rupert and Rivaz, Hassan},
 title = {Fine Tuning U-Net for Ultrasound Image Segmentation: Which Layers?},
 pages = {235--242},
 bookpagination = {page},
 booktitle = {Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data},
 year = {2019}
}


@article{Ankit.04.02.2019,
 author = {Ankit, Utkarsh},
 title = {Semantic Segmentation of Aerial Images Using Deep Learning},
 journaltitle = {Towards Data Science},
 date = {2019-02-04},
 abstract = {Pixel-wise image segmentation is a challenging and demanding task in computer vision and image processing. This blog is about segmentation of Buildings from Aerial (satellite/drone) images$\ldots$},
 file = {Ankit 04.02.2019 - Semantic Segmentation of Aerial Images:Attachments/Ankit 04.02.2019 - Semantic Segmentation of Aerial Images.pdf:application/pdf}
}


@online{AshleshaVaidya.,
 author = {{Ashlesha Vaidya}},
 title = {Semi-Supervised Semantic Segmentation in UAV Imagery},
 url = {https://escholarship.org/uc/item/54z373nh#main},
 urldate = {2022-10-06},
 file = {eScholarship UC item 54z373nh:Attachments/eScholarship UC item 54z373nh.pdf:application/pdf}
}


@inproceedings{Ashwath.10.11.2020,
 author = {Demir, Ilke and Koperski, Krzysztof and Lindenbaum, David and Pang, Guan and Huang, Jing and Basu, Saikat and Hughes, Forest and Tuia, Devis and Raskar, Ramesh},
 title = {Deepglobe 2018: A challenge to Parse the Earth through Satellite Images},
 pages = {172--181},
 bookpagination = {page},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2018}
}


@misc{Azimi.2018,
 author = {Azimi, Seyed Majid and Fischer, Peter and Korner, Marco and Reinartz, Peter},
 year = {2019},
 title = {Aerial LaneNet: Lane-Marking Semantic Segmentation in Aerial Imagery Using Wavelet-Enhanced Cost-Sensitive Symmetric Fully Convolutional Neural Networks},
 url = {https://arxiv.org/pdf/1803.06904},
 number = {5},
 abstract = {The knowledge about the placement and appearance of lane markings is a prerequisite for the creation of maps with high precision, necessary for autonomous driving, infrastructure monitoring, lane-wise traffic management, and urban planning. Lane markings are one of the important components of such maps. Lane markings convey the rules of roads to drivers. While these rules are learned by humans, an autonomous driving vehicle should be taught to learn them to localize itself. Therefore, accurate and reliable lane marking semantic segmentation in the imagery of roads and highways is needed to achieve such goals. We use airborne imagery which can capture a large area in a short period of time by introducing an aerial lane marking dataset. In this work, we propose a Symmetric Fully Convolutional Neural Network enhanced by Wavelet Transform in order to automatically carry out lane marking segmentation in aerial imagery. Due to a heavily unbalanced problem in terms of number of lane marking pixels compared with background pixels, we use a customized loss function as well as a new type of data augmentation step. We achieve a very high accuracy in pixel-wise localization of lane markings without using 3rd-party information. In this work, we introduce the first high-quality dataset used within our experiments which contains a broad range of situations and classes of lane markings representative of current transportation systems. This dataset will be publicly available and hence, it can be used as the benchmark dataset for future algorithms within this domain.},
 doi = {10.1109/TGRS.2018.2878510},
 file = {Azimi, Fischer et al. 2018 - Aerial LaneNet:Attachments/Azimi, Fischer et al. 2018 - Aerial LaneNet.pdf:application/pdf}
}


@article{balraj98.20.01.2021,
 author = {balraj98},
 title = {Road Extraction from Satellite Images [DeepLabV3+]},
 journaltitle = {Kaggle},
 date = {2021-01-20},
 abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from multiple data sources}
}


@misc{Behley.02.04.2019,
 author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Stachniss, Cyrill and Gall, Juergen},
 year = {02.04.2019},
 title = {SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR  Sequences},
 url = {https://arxiv.org/pdf/1904.01416},
 abstract = {Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR.  In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete {\$}360{\^{}}{o}{\$} field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.},
 file = {Behley, Garbade et al. 02.04.2019 - SemanticKITTI:Attachments/Behley, Garbade et al. 02.04.2019 - SemanticKITTI.pdf:application/pdf},
 note = {ICCV2019. See teaser video at http://bit.ly/SemanticKITTI-teaser}
}


@misc{Boguszewski.05.05.2020,
 author = {Boguszewski, Adrian and Batorski, Dominik and Ziemba-Jankowska, Natalia and Dziedzic, Tomasz and Zambrzycka, Anna},
 year = {05.05.2020},
 title = {LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands,  Water and Roads from Aerial Imagery},
 url = {https://arxiv.org/pdf/2005.02264},
 abstract = {Monitoring of land cover and land use is crucial in natural resources management. Automatic visual mapping can carry enormous economic value for agriculture, forestry, or public administration. Satellite or aerial images combined with computer vision and deep learning enable precise assessment and can significantly speed up change detection. Aerial imagery usually provides images with much higher pixel resolution than satellite data allowing more detailed mapping. However, there is still a lack of aerial datasets made for the segmentation, covering rural areas with a resolution of tens centimeters per pixel, manual fine labels, and highly publicly important environmental instances like buildings, woods, water, or roads.  Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for semantic segmentation. We collected images of 216.27 sq. km rural areas across Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine annotated four following classes of objects: buildings, woodlands, water, and roads. Additionally, we report simple benchmark results, achieving 85.56{\%} of mean intersection over union on the test set. It proves that the automatic mapping of land cover is possible with a relatively small, cost-efficient, RGB-only dataset. The dataset is publicly available at https://landcover.ai.linuxpolska.com/},
 file = {Boguszewski, Batorski et al. 05.05.2020 - LandCover.ai:Attachments/Boguszewski, Batorski et al. 05.05.2020 - LandCover.ai.pdf:application/pdf}
}


@article{Brownlee.22.05.2018,
 author = {Brownlee, Jason},
 title = {A Gentle Introduction to k-fold Cross-Validation},
 journaltitle = {Machine Learning Mastery},
 date = {2018-05-22},
 file = {Brownlee 22.05.2018 - A Gentle Introduction to k-fold:Attachments/Brownlee 22.05.2018 - A Gentle Introduction to k-fold.pdf:application/pdf}
}


@inproceedings{C.Henry.2021,
 author = {Henry, Corentin and Fraundorfer, Friedrich and Vig, Eleonora},
 title = {Aerial Road Segmentation in the Presence of Topological Label Noise},
 pages = {2336--2343},
 bookpagination = {page},
 isbn = {1051-4651},
 booktitle = {25th International Conference on Pattern Recognition (ICPR)},
 year = {2020},
 abstract = {The availability of large-scale annotated datasets has enabled Fully-Convolutional Neural Networks to reach outstanding performance on road extraction in aerial images. However, high-quality pixel-level annotation is expensive to produce and even manually labeled data often contains topological errors. Trading off quality for quantity, many datasets rely on already available yet noisy labels, for example from OpenStreetMap. In this paper, we explore the training of custom U-Nets built with ResNet and DenseNet backbones using noise-aware losses that are robust towards label omission and registration noise. We perform an extensive evaluation of standard and noise-aware losses, including a novel Bootstrapped DICE-Coefficient loss, on two challenging road segmentation benchmarks. Our losses yield a consistent improvement in overall extraction quality and exhibit a strong capacity to cope with severe label noise. Our method generalizes well to two other fine-grained topology delineation tasks: surface crack detection for quality inspection and cell membrane extraction in electron microscopy imagery.},
 doi = {10.1109/ICPR48806.2021.9412054},
 file = {C. Henry, F. Fraundorfer et al. 2021 - Aerial Road Segmentation:Attachments/C. Henry, F. Fraundorfer et al. 2021 - Aerial Road Segmentation.pdf:application/pdf},
 eventtitle = {25th International Conference on Pattern Recognition (ICPR)}
}


@article{Cheng.05.10.2021,
 author = {Cheng, Dorothy and Lam, Edmund Y.},
 year = {2021},
 title = {Transfer Learning U-Net Deep Learning for Lung Ultrasound Segmentation},
 journaltitle = {arXiv:2110.02196}
}


@online{ChesapeakeConservancy.02.06.2022,
 author = {{Chesapeake Conservancy}},
 year = {2016},
 title = {Chesapeake Bay Program Land Use/Land Cover Data Project},
 url = {https://www.chesapeakeconservancy.org/conservation-innovation-center/high-resolution-data/lulc-data-project-2022/},
 urldate = {2022-10-06}
}


@article{ChristianWiedemann.1998,
 author = {{Christian Wiedemann} and {Christian Heipke} and {Helmut Mayer} and {Olivier Jamet}},
 year = {1998},
 title = {Empirical Evaluation of Automatically Extracted Road Axes},
 pages = {172--187},
 pagination = {page},
 journaltitle = {Empirical evaluation techniques in computer vision 12},
 abstract = {PDF | Internal self-diagnosis and external evaluation of the obtained results are of major importance for the relevance of any automatic system for... | Find, read and cite all the research you need on ResearchGate},
 file = {Christian Wiedemann, Christian Heipke et al. 1998 - Empirical Evaluation Of Automatically Extracted (2):Attachments/Christian Wiedemann, Christian Heipke et al. 1998 - Empirical Evaluation Of Automatically Extracted (2).pdf:application/pdf}
}


@article{Clevert.23112015,
 author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
 year = {2015},
 title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
 journaltitle = {arXiv:1511.07289}
}


@inproceedings{Constantin.2018,
 author = {Constantin, Alexandre and Ding, Jian-Jiun and Lee, Yih-Cherng},
 title = {Accurate Road Detection from Satellite Images Using Modified U-net},
 pages = {423--426},
 bookpagination = {page},
 isbn = {978-1-5386-8240-1},
 booktitle = {IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
 year = {2018},
 doi = {10.1109/APCCAS.2018.8605652}
}


@book{Cybenko.1999,
 author = {Cybenko, George and O'Leary, Dianne P. and Rissanen, Jorma},
 year = {2012},
 title = {The Mathematics of Information Coding, Extraction, and Distribution},
 volume = {107},
 publisher = {{Springer Science {\&} Business Media}},
 isbn = {0387986650},
 location = {New York},
 series = {The IMA volumes in mathematics and its applications},
 organization = {{University of Minnesota. Institute for Mathematics and Its Applications}}
}


@article{Dice.1945,
 author = {Dice, Lee R.},
 year = {1945},
 title = {Measures of the Amount of Ecologic Association Between Species},
 pages = {297--302},
 pagination = {page},
 volume = {26},
 issn = {00129658},
 journaltitle = {Ecology},
 doi = {10.2307/1932409},
 number = {3}
}


@online{dop27022023,
 author = {N.N.},
 title = {Digitale Orthophotos (DOP)},
 url = {https://www.lgl-bw.de/unsere-themen/Geoinformation/Topographie/Digitale-Orthophotos-DOP/},
 urldate = {2023-02-27},
 organization = {{Landesamt f{\"u}r Geoinformation und Landentwicklung}}
}


@online{Englich.06.10.2022,
 author = {Englich, Markus},
 year = {06.10.2022},
 title = {Benchmark on Semantic Labeling},
 url = {https://www.isprs.org/education/benchmarks/UrbanSemLab/default.aspx},
 urldate = {2022-10-06},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@article{Englich.06.10.2022b,
 author = {Rottensteiner, Franz and Sohn, Gunho and Jung, Jaewook and Gerke, Markus and Baillard, Caroline and Benitez, Sebastien and Breitkopf, Uwe},
 year = {2012},
 title = {The ISPRS Benchmark on Urban Object Classification and 3D Building Reconstruction},
 pages = {293--298},
 pagination = {page},
 journaltitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences}
}


@article{Englich.17.11.2022,
 author = {Gerke, Markus},
 year = {2014},
 title = {Use of the Stair Vision Library within the ISPRS 2D Semantic Labeling Benchmark (Vaihingen)},
 urldate = {2022-11-17},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@online{Englich.17.11.2022b,
 author = {Englich, Markus},
 year = {17.11.2022},
 title = {2D Semantic Labeling Contest - Potsdam},
 url = {https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx},
 urldate = {2022-11-17},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@inproceedings{FaridaBintAhmadNchare.2022,
 author = {{Farida Bint Ahmad Nchare} and {Hippolyte Tapamo}},
 title = {Semantic segmentation of high-resolution aerial imagery using a fully convolutional network},
 url = {https://hal.inria.fr/hal-03715809},
 urldate = {2022-07-06},
 year = {2022},
 abstract = {Semantic segmentation applied to aerial imagery allows the extraction of terrestrial objects such as roads, buildings and even vegetation. Having large, detailed datasets of navigable roads, is of paramount importance in several application fields; namely urban planning, automatic navigation, disaster management. To reach this goal, extracting all roads in a given territory area is the first step. This paper presents a modern method to semantically segment aerial images for a road network extraction. We employ an encoder-decoder architecture to approach the problem of disconnected road regions faced by some existing methods. Using an FCN approach, the localization information was combined to the semantic one, to enable the reconstruction of the road by the proposed model, while being consistent with following the spatial alignment. The method was implemented and evaluated on the public dataset Massassuchets Roads. Results appear to be in full agreement with the theorical predictions and a significant improvement in road connectivity over some previous works; the proposed network achieved a precision of 87.86{\%} and a recall of 87.89{\%}.},
 file = {Farida Bint Ahmad Nchare, Hippolyte Tapamo 2022 - Semantic segmentation of high-resolution aerial:Attachments/Farida Bint Ahmad Nchare, Hippolyte Tapamo 2022 - Semantic segmentation of high-resolution aerial.pdf:application/pdf},
 language = {en},
 eventtitle = {CARI 2022}
}


@article{Fletcher.2018,
 author = {Fletcher, Sam and Islam, Md Zahidul},
 year = {2018},
 title = {Comparing Sets of Patterns with the Jaccard Index},
 volume = {22},
 issn = {1449-8618},
 journaltitle = {Australasian Journal of Information Systems},
 shortjournal = {1},
 language = {en},
 doi = {10.3127/ajis.v22i0.1538},
 abstract = {The Australasian Journal of Information Systems is a refereed journal that publishes articles contributing to Information Systems theory and practice.},
 file = {Fletcher, Islam 2018 - Comparing sets of patterns:Attachments/Fletcher, Islam 2018 - Comparing sets of patterns.pdf:application/pdf}
}


@article{Gao.2019,
 author = {Gao, Lin and Song, Weidong and Dai, Jiguang and Chen, Yang},
 year = {2019},
 title = {Road Extraction from High-Resolution Remote Sensing Imagery Using Refined Deep Residual Convolutional Neural Network},
 pages = {552},
 pagination = {page},
 volume = {11},
 journaltitle = {Remote Sensing},
 doi = {10.3390/rs11050552},
 number = {5},
 file = {Gao, Song et al. 2019 - Road Extraction from High-Resolution Remote:Attachments/Gao, Song et al. 2019 - Road Extraction from High-Resolution Remote.pdf:application/pdf},
 note = {PII:  rs11050552}
}


@article{Geiger.2013,
 author = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
 year = {2013},
 title = {Vision Meets Robotics: The KITTI Dataset},
 pages = {1231--1237},
 pagination = {page},
 volume = {32},
 issn = {0278-3649},
 journaltitle = {The International Journal of Robotics Research},
 doi = {10.1177/0278364913491297},
 number = {11},
 file = {Geiger, Lenz et al. 2013 - Vision meets robotics:Attachments/Geiger, Lenz et al. 2013 - Vision meets robotics.pdf:application/pdf}
}


@online{GitHub.06.10.2022,
 author = {GitHub},
 year = {06.10.2022},
 title = {ayushdabra/drone-images-semantic-segmentation: Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\&}quot},
 url = {https://github.com/ayushdabra/drone-images-semantic-segmentation},
 urldate = {2022-10-06},
 abstract = {Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\textquotedbl} - ayushdabra/drone-images-semantic-segmentation: Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\textquotedbl}}
}


@online{GitHub.06.10.2022b,
 author = {GitHub},
 year = {06.10.2022},
 title = {mahmoudmohsen213/airs: Road Segmentation in Satellite Aerial Images},
 url = {https://github.com/mahmoudmohsen213/airs},
 urldate = {2022-10-06},
 abstract = {Road Segmentation in Satellite Aerial Images. Contribute to mahmoudmohsen213/airs development by creating an account on GitHub.}
}


@online{GitHub.07.10.2022,
 author = {GitHub},
 year = {07.10.2022},
 title = {dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation},
 url = {https://github.com/dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation},
 urldate = {2022-10-07},
 abstract = {Contribute to dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation development by creating an account on GitHub.}
}


@online{GitHub.07.10.2022b,
 author = {GitHub},
 year = {07.10.2022},
 title = {qubvel/segmentation{\_}models: Segmentation models with pretrained backbones. Keras and TensorFlow Keras},
 url = {https://github.com/qubvel/segmentation_models},
 urldate = {2022-10-07},
 abstract = {Segmentation models with pretrained backbones. Keras and TensorFlow Keras. - qubvel/segmentation{\_}models: Segmentation models with pretrained backbones. Keras and TensorFlow Keras.}
}


@book{Goodfellow.2016,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 year = {2016},
 title = {Deep Learning},
 publisher = {{MIT Press}},
 isbn = {9780262035613},
 location = {Cambridge, Massachusetts},
 series = {Adaptive computation and machine learning}
}


@inproceedings{He.10122015,
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 title = {Deep Residual Learning for Image Recognition},
 url = {https://arxiv.org/pdf/1512.03385},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2016},
 abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
 file = {He, Zhang et al. 10 12 2015 - Deep Residual Learning for Image:Attachments/He, Zhang et al. 10 12 2015 - Deep Residual Learning for Image.pdf:application/pdf},
 note = {Tech report}
}


@inproceedings{Huang.25082016,
 author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
 title = {Densely Connected Convolutional Networks},
 url = {https://arxiv.org/pdf/1608.06993},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2017},
 abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
 file = {Huang, Liu et al. 25 08 2016 - Densely Connected Convolutional Networks:Attachments/Huang, Liu et al. 25 08 2016 - Densely Connected Convolutional Networks.pdf:application/pdf},
 note = {CVPR 2017}
}


@online{HumansInTheLoop.29.05.2020,
 author = {{Humans In The Loop}},
 year = {29.05.2020},
 title = {Semantic segmentation of aerial imagery},
 url = {https://www.kaggle.com/datasets/humansintheloop/semantic-segmentation-of-aerial-imagery},
 urldate = {2022-10-06},
 abstract = {{\#}{\#}{\#} Context

Humans in the Loop is publishing an open access dataset annotated for a joint project with the Mohammed Bin Rashid Space Center in Dubai, the UAE.

{\#}{\#}{\#} Content

The dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The total volume of the dataset is 72 images grouped into 6 larger tiles. The classes are:

1. Building: {\#}3C1098

2. Land (unpaved area): {\#}8429F6

3. Road: {\#}6EC1E4

4. Vegetation: {\#}FEDD3A

5. Water: {\#}E2A929

6. Unlabeled: {\#}9B9B9B

{\#}{\#}{\#} Acknowledgements

The images were segmented by the trainees of the Roia Foundation in Syria.}
}


@proceedings{ieee16,
 year = {2016},
 title = {IEEE Conference on Computer Vision and Pattern Recognition}
}


@proceedings{ieee17,
 year = {2017},
 title = {IEEE Conference on Computer Vision and Pattern Recognition}
}


@proceedings{ieee20,
 year = {2020},
 title = {IEEE Conference on Computer Vision and Pattern Recognition}
}


@article{Ioffe.11022015,
 author = {Ioffe, Sergey and Szegedy, Christian},
 year = {2015},
 title = {Batch Normalization: Accelerating Deep Network Training by Reducing  Internal Covariate Shift},
 journaltitle = {International conference on machine learning},
 abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
 file = {Ioffe, Szegedy 11 02 2015 - Batch Normalization:Attachments/Ioffe, Szegedy 11 02 2015 - Batch Normalization.pdf:application/pdf}
}


@misc{Kaiser.2017,
 author = {Kaiser, Pascal and Wegner, Jan Dirk and Lucchi, Aurelien and Jaggi, Martin and Hofmann, Thomas and Schindler, Konrad},
 year = {2017},
 title = {Learning Aerial Image Segmentation From Online Maps},
 url = {https://arxiv.org/pdf/1707.06879},
 number = {11},
 abstract = {This study deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data-hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps which can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale, publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled, pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.},
 doi = {10.1109/TGRS.2017.2719738},
 file = {Kaiser, Wegner et al. 2017 - Learning Aerial Image Segmentation:Attachments/Kaiser, Wegner et al. 2017 - Learning Aerial Image Segmentation.pdf:application/pdf}
}


@inproceedings{Kamiya.2018,
 author = {Kamiya, Ryosuke and Hotta, Kazuhiro and Oda, Kazuo and Kakuta, Satomi},
 title = {Road Detection from Satellite Images by Improving U-Net with Difference of Features},
 pages = {603--607},
 bookpagination = {page},
 isbn = {978-989-758-276-9},
 booktitle = {International Conference on Pattern Recognition Applications and Methods},
 year = {2018},
 doi = {10.5220/0006717506030607},
 eventdate = {16/01/2018 - 18/01/2018}
}


@article{Kovan.05.01.2022,
 author = {Kovan, Ibrahim},
 title = {Implementing U-Net Architecture from scratch, What is U-Net? U-Net with Transfer Learning | Towards Data Science},
 journaltitle = {Towards Data Science},
 date = {2022-01-05},
 abstract = {This article explains the U-Net architecture and includes a real-world project with python implementation. Different coding approaches in computer vision (from scratch and transfer learning). Jaccard Index for computer vision.},
 file = {Kovan 05.01.2022 - Implementing U-Net Architecture from scratch:Attachments/Kovan 05.01.2022 - Implementing U-Net Architecture from scratch.pdf:application/pdf}
}


@thesis{Mnih.2013,
 author = {Mnih, Volodymyr},
 year = {2013},
 title = {Machine Learning for Aerial Image Labeling},
 institution = {{University of Toronto}}
}


@book{Murphy.2012,
 author = {Murphy, Kevin P.},
 year = {2012},
 title = {Machine Learning: A Probabilistic Perspective},
 publisher = {{MIT Press}},
 isbn = {9780262018029},
 location = {Cambridge, Mass. and London},
 titleaddon = {A Probabilistic Perspective},
 series = {Adaptive computation and machine learning series}
}


@online{Nithish.17.04.2022,
 author = {Nithish},
 year = {17.04.2022},
 title = {Satellite Imagery Road Segmentation},
 url = {https://medium.com/@nithishmailme/satellite-imagery-road-segmentation-ad2964dc3812},
 urldate = {2022-10-06},
 file = {Nithish 17.04.2022 - Satellite Imagery Road Segmentation:Attachments/Nithish 17.04.2022 - Satellite Imagery Road Segmentation.pdf:application/pdf}
}


@article{NitishSrivastava.2014,
 author = {{Nitish Srivastava} and {Geoffrey Hinton} and {Alex Krizhevsky} and {Ilya Sutskever} and {Ruslan Salakhutdinov}},
 year = {2014},
 title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
 pages = {1929--1958},
 pagination = {page},
 volume = {15},
 issn = {1532-4435},
 journaltitle = {Journal of Machine Learning Research},
 number = {1},
 abstract = {Request PDF | Dropout: A Simple Way to Prevent Neural Networks from Overfitting | Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such... | Find, read and cite all the research you need on ResearchGate}
}


@inproceedings{O.Filin.2018,
 author = {Filin, O. and Zapara, A. and Panchenko, S.},
 title = {Road Detection with EOSResUNet and Post Vectorizing Algorithm},
 pages = {201--215},
 bookpagination = {page},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2018},
 abstract = {Object recognition on the satellite images is one of the most relevant and popular topics in the problem of pattern recognition. This was facilitated by many factors, such as a high number of satellites with high-resolution imagery, the significant development of computer vision, especially with a major breakthrough in the field of convolutional neural networks, a wide range of industry verticals for usage and still a quite empty market. Roads are one of the most popular objects for recognition. In this article, we want to present you the combination of work of neural network and postprocessing algorithm, due to which we get not only the coverage mask but also the vectors of all of the individual roads that are present in the image and can be used to address the higher-level tasks in the future. This approach was used to solve the DeepGlobe Road Extraction Challenge.},
 doi = {10.1109/CVPRW.2018.00036}
}


@misc{OpenStreetMapcontributors.2017,
 author = {{OpenStreetMap contributors}},
 year = {2023},
 title = {Planet dump retrieved from https://planet.osm.org},
 note = {https://www.openstreetmap.org}
}


@article{Paul.18.12.2019,
 author = {Paul, Jerin},
 title = {Segmentation of Roads in Aerial Images. - Towards Data Science},
 journaltitle = {Towards Data Science},
 date = {2019-12-18},
 abstract = {This comprehensive article will help you to create a road segmentation model, which can detect and segment roads in aerial images.},
 file = {Paul 18.12.2019 - Segmentation of Roads in Aerial:Attachments/Paul 18.12.2019 - Segmentation of Roads in Aerial.pdf:application/pdf}
}


@article{Rasidin.28.07.2020,
 author = {Rasidin, Said},
 title = {Drone Aerial View Segmentation - Analytics Vidhya - Medium},
 journaltitle = {Analytics Vidhya},
 date = {2020-07-28},
 abstract = {Drone uses already gain popularity in the past few years, it provides high resolution images compare to satellite imagery with lower cost, flexibility and low-flying altitude thus leading to$\ldots$},
 file = {Rasidin 28.07.2020 - Drone Aerial View Segmentation:Attachments/Rasidin 28.07.2020 - Drone Aerial View Segmentation.pdf:application/pdf}
}


@inproceedings{Ronneberger.18052015,
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
 pages = {234--241},
 bookpagination = {page},
 booktitle = {Medical Image Computing and Computer-Assisted Intervention Lecture Notes in Computer Science Vol. 9351},
 year = {2015}
}


@online{Ruder.3212017,
 author = {Ruder, Sebastian},
 year = {2017},
 title = {Transfer Learning - Machine Learning's Next Frontier},
 url = {https://ruder.io/transfer-learning/},
 urldate = {2022-11-07},
 abstract = {Deep learning models excel at learning from a large number of labeled examples, but typically do not generalize to conditions not seen during training. This post gives an overview of transfer learning, motivates why it warrants our application, and discusses practical applications and methods.},
 file = {Ruder 3 21 2017 - Transfer Learning:Attachments/Ruder 3 21 2017 - Transfer Learning.pdf:application/pdf}
}


@book{Russell.2012,
 author = {Russell, Stuart J. and Norvig, Peter},
 year = {2012},
 title = {K{\"u}nstliche Intelligenz: Ein moderner Ansatz},
 edition = {3., aktualisierte Aufl.},
 publisher = {{Pearson Studium M{\"u}nchen}},
 isbn = {9783868940985},
 subtitle = {Ein moderner Ansatz},
 language = {ger},
 location = {M{\"u}nchen},
 series = {Pearson-Studium},
 pagetotal = {1307},
 note = {Russell, Stuart (Verfasser.)

Norvig, Peter (Verfasser.)}
}


@article{Sharma.21.08.2019,
 author = {Sharma, Pulkit},
 title = {Image Classification vs. Object Detection vs. Image Segmentation},
 journaltitle = {Analytics Vidhya},
 date = {2019},
 abstract = {The difference between Image Classification, Object Detection and Image Segmentation in the context of Computer Vision},
 file = {Sharma 21.08.2019 - Image Classification vs:Attachments/Sharma 21.08.2019 - Image Classification vs.pdf:application/pdf}
}


@article{Simonyan.04092014,
 author = {Simonyan, Karen and Zisserman, Andrew},
 year = {2014},
 title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
 journaltitle = {arXiv:1409.1556},
 abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
 file = {Simonyan, Zisserman 04 09 2014 - Very Deep Convolutional Networks:Attachments/Simonyan, Zisserman 04 09 2014 - Very Deep Convolutional Networks.pdf:application/pdf}
}


@article{Sorensen.1948,
 author = {Sorensen, Thorvald A.},
 year = {1948},
 title = {A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on Danish commons},
 pages = {1--34},
 pagination = {page},
 volume = {5},
 journaltitle = {Biol. Skar.}
}


@proceedings{Springer.2015,
 year = {2015},
 title = {Medical Image Computing and Computer-Assisted Intervention Lecture Notes in Computer Science Vol. 9351}
}


@proceedings{Springer.2019,
 year = {2019},
 title = {Domain Adaptation and Representation Transfer and Medical Image Learning with Less Labels and Imperfect Data}
}


@inproceedings{Tan.2020,
 author = {Tan, Weikai and Qin, Nannan and Ma, Lingfei and Li, Ying and Du, Jing and Cai, Guorong and Yang, Ke and Li, Jonathan},
 title = {Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways},
 url = {https://arxiv.org/pdf/2003.08284},
 pages = {202--203},
 bookpagination = {page},
 booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2020},
 abstract = {Semantic segmentation of large-scale outdoor point clouds is essential for urban scene understanding in various applications, especially autonomous driving and urban high-definition (HD) mapping. With rapid developments of mobile laser scanning (MLS) systems, massive point clouds are available for scene understanding, but publicly accessible large-scale labeled datasets, which are essential for developing learning-based methods, are still limited. This paper introduces Toronto-3D, a large-scale urban outdoor point cloud dataset acquired by a MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of point clouds and consists of about 78.3 million points with 8 labeled object classes. Baseline experiments for semantic segmentation were conducted and the results confirmed the capability of this dataset to train deep learning models effectively. Toronto-3D is released to encourage new research, and the labels will be improved and updated with feedback from the research community.},
 doi = {10.1109/CVPRW50498.2020.00109},
 file = {Tan, Qin et al. 2020 - Toronto-3D:Attachments/Tan, Qin et al. 2020 - Toronto-3D.pdf:application/pdf}
}


@article{Yerram.2022,
 author = {Yerram, Varun and Takeshita, Hiroyuki and Iwahori, Yuji and Hayashi, Yoshitsugu and Bhuyan, M. K. and Fukui, Shinji and Kijsirikul, Boonserm and Wang, Aili},
 year = {2022},
 title = {Extraction and Calculation of Roadway Area from Satellite Images Using Improved Deep Learning Model and Post-Processing},
 pages = {124},
 pagination = {page},
 volume = {8},
 journaltitle = {Journal of Imaging},
 shortjournal = {J. Imaging},
 language = {eng},
 doi = {10.3390/jimaging8050124},
 number = {5},
 abstract = {Roadway area calculation is a novel problem in remote sensing and urban planning. This paper models this problem as a two-step problem, roadway extraction, and area calculation. Roadway extraction from satellite images is a problem that has been tackled many times before. This paper proposes a method using pixel resolution to calculate the area of the roads covered in satellite images. The proposed approach uses novel U-net and Resnet architectures called U-net++ and ResNeXt. The state-of-the-art model is combined with the proposed efficient post-processing approach to improve the overlap with ground truth labels. The performance of the proposed road extraction algorithm is evaluated on the Massachusetts dataset and it is shown that the proposed approach outperforms the existing solutions which use models from the U-net family.},
 file = {Yerram, Takeshita et al. 2022 - Extraction and Calculation of Roadway:Attachments/Yerram, Takeshita et al. 2022 - Extraction and Calculation of Roadway.pdf:application/pdf;Yerram, Takeshita et al. 2022 - Extraction and Calculation of Roadway (2):Attachments/Yerram, Takeshita et al. 2022 - Extraction and Calculation of Roadway (2).pdf:application/pdf},
 note = {PII:  jimaging8050124

Journal Article},
 eprint = {35621888}
}


@article{YutakaSasaki.2007,
 author = {Sasaki, Yutaka},
 year = {2007},
 title = {The Truth of the F-Measure},
 pages = {1--5},
 pagination = {page},
 journaltitle = {Teach Tutor Mater},
 note = {01}
}


