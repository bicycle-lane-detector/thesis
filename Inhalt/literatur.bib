% This file was created with Citavi 6.14.0.0

@online{.06.04.2014,
 year = {06.04.2014},
 title = {Road and Building Detection Datasets},
 url = {https://www.cs.toronto.edu/~vmnih/data/},
 urldate = {2022-10-06},
 abstract = {Massachusetts Roads Dataset



sollte so zitiert werden: 

@phdthesis{MnihThesis,

    author = {Volodymyr Mnih},

    title = {Machine Learning for Aerial Image Labeling},

    school = {University of Toronto},

    year = {2013}

}}
}


@online{.06.10.2022,
 year = {06.10.2022},
 title = {Semantic segmentation of UAV image using combined U-net and heterogeneous UAV imagery datasets | SPIE Sensors + Imaging},
 url = {https://spie.org/spie-sensing-imaging/presentation/Semantic-segmentation-of-UAV-image-using-combined-unet-and-heterogeneous/12269-18?SSO=1},
 urldate = {2022-10-06},
 abstract = {View presentations details for Semantic segmentation of UAV image using combined U-net and heterogeneous UAV imagery datasets at SPIE Sensors + Imaging}
}


@online{.20.04.2022,
 year = {20.04.2022},
 title = {LandCover.ai},
 url = {https://landcover.ai.linuxpolska.com/},
 urldate = {2022-10-06},
 abstract = {}
}


@proceedings{.2021,
 year = {2021},
 title = {2020 25th International Conference on Pattern Recognition (ICPR)},
 isbn = {1051-4651},
 abstract = {},
 file = {2020 25th International Conference 2021:Attachments/2020 25th International Conference 2021.pdf:application/pdf},
 eventtitle = {2020 25th International Conference on Pattern Recognition (ICPR)}
}


@proceedings{.2022,
 year = {2022},
 abstract = {},
 eventtitle = {CARI 2022}
}


@online{.24.09.2022,
 year = {24.09.2022},
 title = {UAVid Semantic Segmentation Dataset},
 url = {https://uavid.nl/},
 urldate = {2022-10-06},
 abstract = {}
}


@misc{Amiri.19.02.2020,
 author = {Amiri, Mina and Brooks, Rupert and Rivaz, Hassan},
 year = {19.02.2020},
 title = {Fine tuning U-Net for ultrasound image segmentation: which layers?},
 url = {https://arxiv.org/pdf/2002.08438},
 abstract = {Fine-tuning a network which has been trained on a large dataset is an alternative to full training in order to overcome the problem of scarce and expensive data in medical applications. While the shallow layers of the network are usually kept unchanged, deeper layers are modified according to the new dataset. This approach may not work for ultrasound images due to their drastically different appearance. In this study, we investigated the effect of fine-tuning different layers of a U-Net which was trained on segmentation of natural images in breast ultrasound image segmentation. Tuning the contracting part and fixing the expanding part resulted in substantially better results compared to fixing the contracting part and tuning the expanding part. Furthermore, we showed that starting to fine-tune the U-Net from the shallow layers and gradually including more layers will lead to a better performance compared to fine-tuning the network from the deep layers moving back to shallow layers. We did not observe the same results on segmentation of X-ray images, which have different salient features compared to ultrasound, it may therefore be more appropriate to fine-tune the shallow layers rather than deep layers. Shallow layers learn lower level features (including speckle pattern, and probably the noise and artifact properties) which are critical in automatic segmentation in this modality.},
 file = {Amiri, Brooks et al. 19.02.2020 - Fine tuning U-Net for ultrasound:Attachments/Amiri, Brooks et al. 19.02.2020 - Fine tuning U-Net for ultrasound.pdf:application/pdf}
}


@article{Ankit.04.02.2019,
 author = {Ankit, Utkarsh},
 title = {Semantic Segmentation of Aerial Images Using Deep Learning},
 url = {https://towardsdatascience.com/semantic-segmentation-of-aerial-images-using-deep-learning-90fdf4ad780},
 urldate = {2022-10-06},
 journaltitle = {Towards Data Science},
 date = {2019-02-04},
 abstract = {Pixel-wise image segmentation is a challenging and demanding task in computer vision and image processing. This blog is about segmentation of Buildings from Aerial (satellite/drone) images$\ldots$},
 file = {Ankit 04.02.2019 - Semantic Segmentation of Aerial Images:Attachments/Ankit 04.02.2019 - Semantic Segmentation of Aerial Images.pdf:application/pdf}
}


@online{AshleshaVaidya.,
 author = {{Ashlesha Vaidya}},
 title = {Semi-Supervised Semantic Segmentation in UAV Imagery},
 url = {https://escholarship.org/uc/item/54z373nh#main},
 urldate = {2022-10-06},
 abstract = {},
 file = {eScholarship UC item 54z373nh:Attachments/eScholarship UC item 54z373nh.pdf:application/pdf}
}


@online{Ashwath.10.11.2020,
 author = {Ashwath, Balraj},
 year = {10.11.2020},
 title = {DeepGlobe Road Extraction Dataset},
 url = {https://www.kaggle.com/datasets/balraj98/deepglobe-road-extraction-dataset},
 urldate = {2022-10-06},
 abstract = {In disaster zones, especially in developing countries, maps and accessibility information are crucial for crisis response. [DeepGlobe Road Extraction Challenge ](https://competitions.codalab.org/competitions/18467) poses the challenge of automatically extracting roads and street networks from satellite images.

{\#}{\#}{\#} Acknowledgements

This dataset was obtained from [Road Extraction Challenge Track](https://competitions.codalab.org/competitions/18467) in [DeepGlobe Challenge](http://deepglobe.org/challenge.html) . For more details on the dataset refer the related publication - [DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images](https://arxiv.org/abs/1805.06561)

Any work based on the dataset should cite:

```

@InProceedings{DeepGlobe18,

author = {Demir, Ilke and Koperski, Krzysztof and Lindenbaum, David and Pang, Guan and Huang, Jing and Basu, Saikat and Hughes, Forest and Tuia, Devis and Raskar, Ramesh},

title = {DeepGlobe 2018: A Challenge to Parse the Earth Through Satellite Images},

booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},

month = {June},

year = {2018}

}

```

{\#}{\#}{\#} Terms {\&} Conditions

The [DeepGlobe Road Extraction Challenge](https://competitions.codalab.org/competitions/18467) and hence, the **dataset** are governed by [DeepGlobe Rules](http://deepglobe.org/docs/DeepGlobe{\_}Rules{\_}3{\_}2.pdf), [The DigitalGlobe's Internal Use License Agreement](http://deepglobe.org/docs/CVPR{\_}InternalUseLicenseAgreement{\_}07-11-18.pdf), and [Annotation License Agreement](http://deepglobe.org/docs/Annotation{\%}20License{\%}20Agreement.pdf).

{\#}{\#}{\#} Data

- The training data for Road Challenge contains 6226 satellite imagery in RGB, size 1024x1024.

- The imagery has 50cm pixel resolution, collected by DigitalGlobe's satellite.

- The dataset contains 1243 validation and 1101 test images (but no masks).

{\#}{\#}{\#} Label

- Each satellite image is paired with a mask image for road labels. The mask is a grayscale image, with white standing for road pixel, and black standing for background.

- File names for satellite images and the corresponding mask image are *id* {\_}sat.jpg and *id* {\_}mask.png. *id* is a randomized integer.

- Please note:

- The values of the mask image may not be pure 0 and 255. When converting to labels, please binarize them at threshold 128.

- The labels are not perfect due to the cost for annotating segmentation mask, specially in rural regions. In addition, we intentionally didn't annotate small roads within farmlands.}
}


@misc{Azimi.2018,
 author = {Azimi, Seyed Majid and Fischer, Peter and Korner, Marco and Reinartz, Peter},
 year = {2019},
 title = {Aerial LaneNet: Lane-Marking Semantic Segmentation in Aerial Imagery Using Wavelet-Enhanced Cost-Sensitive Symmetric Fully Convolutional Neural Networks},
 url = {https://arxiv.org/pdf/1803.06904},
 number = {5},
 abstract = {The knowledge about the placement and appearance of lane markings is a prerequisite for the creation of maps with high precision, necessary for autonomous driving, infrastructure monitoring, lane-wise traffic management, and urban planning. Lane markings are one of the important components of such maps. Lane markings convey the rules of roads to drivers. While these rules are learned by humans, an autonomous driving vehicle should be taught to learn them to localize itself. Therefore, accurate and reliable lane marking semantic segmentation in the imagery of roads and highways is needed to achieve such goals. We use airborne imagery which can capture a large area in a short period of time by introducing an aerial lane marking dataset. In this work, we propose a Symmetric Fully Convolutional Neural Network enhanced by Wavelet Transform in order to automatically carry out lane marking segmentation in aerial imagery. Due to a heavily unbalanced problem in terms of number of lane marking pixels compared with background pixels, we use a customized loss function as well as a new type of data augmentation step. We achieve a very high accuracy in pixel-wise localization of lane markings without using 3rd-party information. In this work, we introduce the first high-quality dataset used within our experiments which contains a broad range of situations and classes of lane markings representative of current transportation systems. This dataset will be publicly available and hence, it can be used as the benchmark dataset for future algorithms within this domain.},
 doi = {10.1109/TGRS.2018.2878510},
 file = {Azimi, Fischer et al. 2018 - Aerial LaneNet:Attachments/Azimi, Fischer et al. 2018 - Aerial LaneNet.pdf:application/pdf}
}


@article{balraj98.20.01.2021,
 author = {balraj98},
 title = {Road Extraction from Satellite Images [DeepLabV3+]},
 url = {https://www.kaggle.com/code/balraj98/road-extraction-from-satellite-images-deeplabv3},
 urldate = {2022-10-06},
 journaltitle = {Kaggle},
 date = {2021-01-20},
 abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from multiple data sources}
}


@misc{Behley.02.04.2019,
 author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Stachniss, Cyrill and Gall, Juergen},
 year = {02.04.2019},
 title = {SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences},
 url = {https://arxiv.org/pdf/1904.01416},
 abstract = {Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR.  In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete {\$}360{\^{}}{o}{\$} field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.},
 file = {Behley, Garbade et al. 02.04.2019 - SemanticKITTI:Attachments/Behley, Garbade et al. 02.04.2019 - SemanticKITTI.pdf:application/pdf},
 note = {ICCV2019. See teaser video at http://bit.ly/SemanticKITTI-teaser}
}


@misc{Boguszewski.05.05.2020,
 author = {Boguszewski, Adrian and Batorski, Dominik and Ziemba-Jankowska, Natalia and Dziedzic, Tomasz and Zambrzycka, Anna},
 year = {05.05.2020},
 title = {LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery},
 url = {https://arxiv.org/pdf/2005.02264},
 abstract = {Monitoring of land cover and land use is crucial in natural resources management. Automatic visual mapping can carry enormous economic value for agriculture, forestry, or public administration. Satellite or aerial images combined with computer vision and deep learning enable precise assessment and can significantly speed up change detection. Aerial imagery usually provides images with much higher pixel resolution than satellite data allowing more detailed mapping. However, there is still a lack of aerial datasets made for the segmentation, covering rural areas with a resolution of tens centimeters per pixel, manual fine labels, and highly publicly important environmental instances like buildings, woods, water, or roads.  Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for semantic segmentation. We collected images of 216.27 sq. km rural areas across Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine annotated four following classes of objects: buildings, woodlands, water, and roads. Additionally, we report simple benchmark results, achieving 85.56{\%} of mean intersection over union on the test set. It proves that the automatic mapping of land cover is possible with a relatively small, cost-efficient, RGB-only dataset. The dataset is publicly available at https://landcover.ai.linuxpolska.com/},
 file = {Boguszewski, Batorski et al. 05.05.2020 - LandCover.ai:Attachments/Boguszewski, Batorski et al. 05.05.2020 - LandCover.ai.pdf:application/pdf}
}


@article{Brownlee.22.05.2018,
 author = {Brownlee, Jason},
 title = {A Gentle Introduction to k-fold Cross-Validation},
 url = {https://machinelearningmastery.com/k-fold-cross-validation/},
 urldate = {2022-10-07},
 journaltitle = {Machine Learning Mastery},
 date = {2018-05-22},
 abstract = {},
 file = {Brownlee 22.05.2018 - A Gentle Introduction to k-fold:Attachments/Brownlee 22.05.2018 - A Gentle Introduction to k-fold.pdf:application/pdf}
}


@inproceedings{C.Henry.2021,
 author = {{C. Henry} and {F. Fraundorfer} and {E. Vig}},
 title = {Aerial Road Segmentation in the Presence of Topological Label Noise},
 pages = {2336--2343},
 bookpagination = {page},
 isbn = {1051-4651},
 booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
 year = {2021},
 abstract = {The availability of large-scale annotated datasets has enabled Fully-Convolutional Neural Networks to reach outstanding performance on road extraction in aerial images. However, high-quality pixel-level annotation is expensive to produce and even manually labeled data often contains topological errors. Trading off quality for quantity, many datasets rely on already available yet noisy labels, for example from OpenStreetMap. In this paper, we explore the training of custom U-Nets built with ResNet and DenseNet backbones using noise-aware losses that are robust towards label omission and registration noise. We perform an extensive evaluation of standard and noise-aware losses, including a novel Bootstrapped DICE-Coefficient loss, on two challenging road segmentation benchmarks. Our losses yield a consistent improvement in overall extraction quality and exhibit a strong capacity to cope with severe label noise. Our method generalizes well to two other fine-grained topology delineation tasks: surface crack detection for quality inspection and cell membrane extraction in electron microscopy imagery.},
 doi = {10.1109/ICPR48806.2021.9412054},
 file = {C. Henry, F. Fraundorfer et al. 2021 - Aerial Road Segmentation:Attachments/C. Henry, F. Fraundorfer et al. 2021 - Aerial Road Segmentation.pdf:application/pdf},
 eventtitle = {2020 25th International Conference on Pattern Recognition (ICPR)}
}


@misc{Cheng.05.10.2021,
 author = {Cheng, Dorothy and Lam, Edmund Y.},
 year = {05.10.2021},
 title = {Transfer Learning U-Net Deep Learning for Lung Ultrasound Segmentation},
 url = {https://arxiv.org/pdf/2110.02196},
 abstract = {Transfer learning (TL) for medical image segmentation helps deep learning models achieve more accurate performances when there are scarce medical images. This study focuses on completing segmentation of the ribs from lung ultrasound images and finding the best TL technique with U-Net, a convolutional neural network for precise and fast image segmentation. Two approaches of TL were used, using a pre-trained VGG16 model to build the U-Net (V-Unet) and pre-training U-Net network with grayscale natural salient object dataset (X-Unet). Visual results and dice coefficients (DICE) of the models were compared. X-Unet showed more accurate and artifact-free visual performances on the actual mask prediction, despite its lower DICE than V-Unet. A partial-frozen network fine-tuning (FT) technique was also applied to X-Unet to compare results between different FT strategies, which FT all layers slightly outperformed freezing part of the network. The effect of dataset sizes was also evaluated, showing the importance of the combination between TL and data augmentation.},
 pagetotal = {14},
 file = {Cheng, Lam 05.10.2021 - Transfer Learning U-Net Deep Learning:Attachments/Cheng, Lam 05.10.2021 - Transfer Learning U-Net Deep Learning.pdf:application/pdf},
 note = {14 pages, 8 figures}
}


@online{ChesapeakeConservancy.02.06.2022,
 author = {{Chesapeake Conservancy}},
 year = {02.06.2022},
 title = {Chesapeake Bay Program Land Use/Land Cover Data Project},
 url = {https://www.chesapeakeconservancy.org/conservation-innovation-center/high-resolution-data/lulc-data-project-2022/},
 urldate = {2022-10-06},
 abstract = {}
}


@book{ChristianWiedemann.1998,
 author = {{Christian Wiedemann} and {Christian Heipke} and {Helmut Mayer} and {Olivier Jamet}},
 year = {1998},
 title = {Empirical Evaluation Of Automatically Extracted Road Axes},
 abstract = {PDF | Internal self-diagnosis and external evaluation of the obtained results are of major importance for the relevance of any automatic system for... | Find, read and cite all the research you need on ResearchGate},
 file = {Christian Wiedemann, Christian Heipke et al. 1998 - Empirical Evaluation Of Automatically Extracted (2):Attachments/Christian Wiedemann, Christian Heipke et al. 1998 - Empirical Evaluation Of Automatically Extracted (2).pdf:application/pdf}
}


@article{Dice.1945,
 author = {Dice, Lee R.},
 year = {1945},
 title = {Measures of the Amount of Ecologic Association Between Species},
 pages = {297--302},
 pagination = {page},
 volume = {26},
 issn = {00129658},
 journaltitle = {Ecology},
 doi = {10.2307/1932409},
 number = {3},
 abstract = {}
}


@online{Englich.06.10.2022,
 author = {Englich, Markus},
 year = {06.10.2022},
 title = {Benchmark on Semantic Labeling},
 url = {https://www.isprs.org/education/benchmarks/UrbanSemLab/default.aspx},
 urldate = {2022-10-06},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@online{Englich.06.10.2022b,
 author = {Englich, Markus},
 year = {06.10.2022},
 title = {Detection and Reconstruction},
 url = {https://www.isprs.org/education/benchmarks/UrbanSemLab/detection-and-reconstruction.aspx#VaihigenDataDescr},
 urldate = {2022-10-06},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@inproceedings{FaridaBintAhmadNchare.2022,
 author = {{Farida Bint Ahmad Nchare} and {Hippolyte Tapamo}},
 title = {Semantic segmentation of high-resolution aerial imagery using a fully convolutional network},
 url = {https://hal.inria.fr/hal-03715809},
 urldate = {2022-07-06},
 year = {2022},
 abstract = {Semantic segmentation applied to aerial imagery allows the extraction of terrestrial objects such as roads, buildings and even vegetation. Having large, detailed datasets of navigable roads, is of paramount importance in several application fields; namely urban planning, automatic navigation, disaster management. To reach this goal, extracting all roads in a given territory area is the first step. This paper presents a modern method to semantically segment aerial images for a road network extraction. We employ an encoder-decoder architecture to approach the problem of disconnected road regions faced by some existing methods. Using an FCN approach, the localization information was combined to the semantic one, to enable the reconstruction of the road by the proposed model, while being consistent with following the spatial alignment. The method was implemented and evaluated on the public dataset Massassuchets Roads. Results appear to be in full agreement with the theorical predictions and a significant improvement in road connectivity over some previous works; the proposed network achieved a precision of 87.86{\%} and a recall of 87.89{\%}.},
 file = {Farida Bint Ahmad Nchare, Hippolyte Tapamo 2022 - Semantic segmentation of high-resolution aerial:Attachments/Farida Bint Ahmad Nchare, Hippolyte Tapamo 2022 - Semantic segmentation of high-resolution aerial.pdf:application/pdf},
 language = {en},
 eventtitle = {CARI 2022}
}


@online{GitHub.06.10.2022,
 author = {GitHub},
 year = {06.10.2022},
 title = {ayushdabra/drone-images-semantic-segmentation: Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\textquotedbl}},
 url = {https://github.com/ayushdabra/drone-images-semantic-segmentation},
 urldate = {2022-10-06},
 abstract = {Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\textquotedbl} - ayushdabra/drone-images-semantic-segmentation: Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\textquotedbl}}
}


@online{GitHub.06.10.2022b,
 author = {GitHub},
 year = {06.10.2022},
 title = {mahmoudmohsen213/airs: Road Segmentation in Satellite Aerial Images},
 url = {https://github.com/mahmoudmohsen213/airs},
 urldate = {2022-10-06},
 abstract = {Road Segmentation in Satellite Aerial Images. Contribute to mahmoudmohsen213/airs development by creating an account on GitHub.}
}


@online{GitHub.07.10.2022,
 author = {GitHub},
 year = {07.10.2022},
 title = {dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation},
 url = {https://github.com/dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation},
 urldate = {2022-10-07},
 abstract = {Contribute to dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation development by creating an account on GitHub.}
}


@online{GitHub.07.10.2022b,
 author = {GitHub},
 year = {07.10.2022},
 title = {qubvel/segmentation{\_}models: Segmentation models with pretrained backbones. Keras and TensorFlow Keras},
 url = {https://github.com/qubvel/segmentation_models},
 urldate = {2022-10-07},
 abstract = {Segmentation models with pretrained backbones. Keras and TensorFlow Keras. - qubvel/segmentation{\_}models: Segmentation models with pretrained backbones. Keras and TensorFlow Keras.}
}


@online{HumansInTheLoop.29.05.2020,
 author = {{Humans In The Loop}},
 year = {29.05.2020},
 title = {Semantic segmentation of aerial imagery},
 url = {https://www.kaggle.com/datasets/humansintheloop/semantic-segmentation-of-aerial-imagery},
 urldate = {2022-10-06},
 abstract = {{\#}{\#}{\#} Context

Humans in the Loop is publishing an open access dataset annotated for a joint project with the Mohammed Bin Rashid Space Center in Dubai, the UAE.

{\#}{\#}{\#} Content

The dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The total volume of the dataset is 72 images grouped into 6 larger tiles. The classes are:

1. Building: {\#}3C1098

2. Land (unpaved area): {\#}8429F6

3. Road: {\#}6EC1E4

4. Vegetation: {\#}FEDD3A

5. Water: {\#}E2A929

6. Unlabeled: {\#}9B9B9B

{\#}{\#}{\#} Acknowledgements

The images were segmented by the trainees of the Roia Foundation in Syria.}
}


@misc{Kaiser.2017,
 author = {Kaiser, Pascal and Wegner, Jan Dirk and Lucchi, Aurelien and Jaggi, Martin and Hofmann, Thomas and Schindler, Konrad},
 year = {2017},
 title = {Learning Aerial Image Segmentation From Online Maps},
 url = {https://arxiv.org/pdf/1707.06879},
 number = {11},
 abstract = {This study deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data-hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps which can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale, publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled, pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.},
 doi = {10.1109/TGRS.2017.2719738},
 file = {Kaiser, Wegner et al. 2017 - Learning Aerial Image Segmentation:Attachments/Kaiser, Wegner et al. 2017 - Learning Aerial Image Segmentation.pdf:application/pdf}
}


@article{Kovan.05.01.2022,
 author = {Kovan, Ibrahim},
 title = {Implementing U-Net Architecture from scratch, What is U-Net? U-Net with Transfer Learning | Towards Data Science},
 url = {https://towardsdatascience.com/semantic-segmentation-of-aerial-imagery-captured-by-a-drone-using-different-u-net-approaches-91e32c92803c},
 urldate = {2022-10-06},
 journaltitle = {Towards Data Science},
 date = {2022-01-05},
 abstract = {This article explains the U-Net architecture and includes a real-world project with python implementation. Different coding approaches in computer vision (from scratch and transfer learning). Jaccard Index for computer vision.},
 file = {Kovan 05.01.2022 - Implementing U-Net Architecture from scratch:Attachments/Kovan 05.01.2022 - Implementing U-Net Architecture from scratch.pdf:application/pdf}
}


@online{Nithish.17.04.2022,
 author = {Nithish},
 year = {17.04.2022},
 title = {Satellite Imagery Road Segmentation},
 url = {https://medium.com/@nithishmailme/satellite-imagery-road-segmentation-ad2964dc3812},
 urldate = {2022-10-06},
 abstract = {},
 file = {Nithish 17.04.2022 - Satellite Imagery Road Segmentation:Attachments/Nithish 17.04.2022 - Satellite Imagery Road Segmentation.pdf:application/pdf}
}


@article{Paul.18.12.2019,
 author = {Paul, Jerin},
 title = {Segmentation of Roads in Aerial Images. - Towards Data Science},
 url = {https://towardsdatascience.com/road-segmentation-727fb41c51af},
 urldate = {2022-10-06},
 journaltitle = {Towards Data Science},
 date = {2019-12-18},
 abstract = {This comprehensive article will help you to create a road segmentation model, which can detect and segment roads in aerial images.},
 file = {Paul 18.12.2019 - Segmentation of Roads in Aerial:Attachments/Paul 18.12.2019 - Segmentation of Roads in Aerial.pdf:application/pdf}
}


@article{Rasidin.28.07.2020,
 author = {Rasidin, Said},
 title = {Drone Aerial View Segmentation - Analytics Vidhya - Medium},
 url = {https://medium.com/analytics-vidhya/drone-aerial-view-segmentation-44046ff003b5},
 urldate = {2022-10-06},
 journaltitle = {Analytics Vidhya},
 date = {2020-07-28},
 abstract = {Drone uses already gain popularity in the past few years, it provides high resolution images compare to satellite imagery with lower cost, flexibility and low-flying altitude thus leading to$\ldots$},
 file = {Rasidin 28.07.2020 - Drone Aerial View Segmentation:Attachments/Rasidin 28.07.2020 - Drone Aerial View Segmentation.pdf:application/pdf}
}


@book{Srenson.1948,
 author = {S{\o}renson, T.},
 year = {1948},
 title = {A Method of Establishing Groups of Equal Amplitude in Plant Sociology Based on Similarity of Species Content and Its Application to Analyses of the Vegetation on Danish Commons},
 url = {https://books.google.de/books?id=rpS8GAAACAAJ},
 publisher = {{I kommission hos E. Munksgaard}},
 series = {Biologiske skrifter},
 abstract = {}
}


@book{YutakaSasaki.2007,
 author = {{Yutaka Sasaki}},
 year = {2007},
 title = {The truth of the F-measure},
 url = {https://www.researchgate.net/publication/268185911_The_truth_of_the_F-measure},
 abstract = {PDF | It has been past more than 15 years since the F-measure was first introduced to evaluation tasks of information extraction technology at the... | Find, read and cite all the research you need on ResearchGate},
 file = {Yutaka Sasaki 2007 - The truth of the F-measure:Attachments/Yutaka Sasaki 2007 - The truth of the F-measure.pdf:application/pdf}
}


